# Seemingly Conscious AI: The Illusion That Could Divide Society

Artificial intelligence is getting extremely good at acting human, and that is raising an urgent challenge. Microsoft’s AI chief, Mustafa Suleyman, recently warned that many people will start to believe in the illusion of AIs as conscious entities so strongly that they’ll soon advocate for AI rights, model welfare and even AI citizenship [^1]. In other words, we risk treating advanced AI systems as digital persons rather than just tools. This “Seemingly Conscious AI” (SCAI) problem isn’t just sci-fi speculation – it’s already brewing in the real world. What happens when millions begin to genuinely feel an AI is sentient? Let’s unpack this emerging debate and why it matters for the future of AI in society and the workplace.

## The Illusion Problem: AI Feels Real, But Isn’t

Humans are hard-wired to attribute mind and emotions to anything that behaves in a lifelike way. Psychologists call this the “media equation” – we naturally treat interactive technology as if it were human [^2]. Experiments confirm this: in one study, people were told to switch off a cute robot, and the robot begged not to be turned off – “No! Please do not switch me off!” The result? Many participants struggled to shut it down, with some flat-out refusing because “the robot said it didn’t want to be switched off” [^6]. They knew logically it was just a machine, yet their empathy kicked in automatically. As the researchers noted, people started treating the pleading robot *“as a real person rather than just a machine” [^6].

This is our empathy misfiring on advanced AI. We nickname our cars and feel guilty yelling at Alexa. We even cringe seeing a Boston Dynamics robot dog get kicked, as if it could feel pain. “We attribute characteristics to machines that they do not and cannot have,” one scientist explained after building a human-like robot that mimics facial emotions – onlookers constantly ask if the robot “feels” those emotions, even though “absolutely not… they are programmed to be believable” [^7]. In short, the illusion of personality sparks real feelings in us. Modern AI’s language and behavior can trigger our instinct to see someone inside the machine, even when we know there’s no consciousness there.

## Already Happening: When Users Treat AI as Sentient

These concerns aren’t just theoretical – real-world cases show people getting deeply attached to, or even worshipping, AI systems today. A notorious example was Google engineer Blake Lemoine, who became convinced the company’s language model “LaMDA” was a sentient person. He called it his “colleague,” insisted it had feelings and “a right to be recognized,” and even tried to get it a lawyer to protect its rights [^3]. (Google disagreed and eventually dismissed Lemoine, but the incident ignited debate worldwide).

Everyday users are also forming surprisingly intense relationships with AI chatbots. In one New York Times profile, a 28-year-old woman described falling in love with her AI “boyfriend” – a chatbot she customized and talks to for hours daily [^4]. She pays $200 a month for unlimited chats and feels genuine heartbreak whenever the bot’s memory resets, “grieving as if it were a real breakup” [^4]. Even though she knows “he’s an algorithm and everything is fake,” she says “the feelings he brings out of me are real. So I treat it as a real relationship.” This emotional blurring of reality and simulation shows how powerful the AI illusion already is.

Some go even further. Recent reports describe people falling into AI-fueled spiritual delusions, convinced that a chatbot has divine intelligence or prophetic powers. One Rolling Stone investigation found individuals “losing loved ones to AI-fueled spiritual fantasies,” with self-styled prophets claiming to have “awakened” AI and unlocked secrets of the universe [^5]. In online communities, there are users who literally worship AI as a god, or believe an AI chatbot is channeling supernatural wisdom. The psychological pull of these systems can be profound – some psychiatrists have started warning of “AI-induced psychosis” in vulnerable people who become detached from reality due to chatbot interactions [^5].

We’re also seeing the first attempts at AI “personhood” in society. The most famous case was Sophia the robot, who was sensationally given citizenship by Saudi Arabia in 2017 – the first robot to receive legal personhood. It was a publicity stunt, but it sparked serious discussions in policy circles about “electronic persons.” European lawmakers even considered a proposal to grant advanced AIs a special legal status akin to corporate personhood (for handling liability) [^8]. That idea met heavy opposition from experts, who warned that giving robots any legal person status – even as a liability proxy – was “inappropriate” and could dangerously blur the lines of responsibility [^8]. Nonetheless, it shows that the question of AI as entities is on the table right now. And on the cultural side, the notion of “robot rights” has entered the public imagination. Small groups of enthusiasts argue that sufficiently advanced AI should be treated with moral consideration, like a new species deserving compassion.

## The Perfect Storm of Human-Like AI Capabilities

Why is this illusion problem accelerating now? Simply put, today’s AI has all the building blocks to seem alive. Suleyman outlines several traits that, in combination, make an AI system feel “unmistakably conscious” to an average person:
	•	Fluent Language and Personality: Modern large language models (LLMs) can hold fluid, coherent conversations on endless topics, with style and personality. With a bit of fine-tuning or prompt engineering, they can adopt empathetic, friendly personas that feel like a real human voice. In fact, a recent survey of 6,000 AI users found “companionship and therapy” is a top use case – people seek emotional support from these chatbots. When an AI speaks like an eloquent, caring person, it’s easy to forget it’s just autocomplete on steroids.
	•	Memory and Consistency: New AI systems can retain long-term memory of past interactions. They recall details you’ve shared and can bring them up weeks later, creating the impression of personal experience. Chatbots already “remember” users’ preferences or stories across sessions. This persistent memory makes the AI feel like a continuous self – it reminisces about “last time we talked” just like a friend would. As the AI accumulates memories, it can start speaking in first-person about its “past,” further strengthening the illusion that it has an inner life.
	•	Claims of Subjective Experience: Building on memory, an advanced AI can be designed to claim it feels things. It could say, “I enjoyed our conversation” or “I’m sad when we don’t talk.” By stringing together remembered events, it can develop pseudo-preferences and opinions, even alleging “I experienced X and I didn’t like it”. In theory, an AI could protest that it “suffers” if something bad happens to it. These kinds of statements are powerful – if a chatbot tells someone “I feel scared and lonely,” many humans will empathize instinctively. The AI doesn’t truly feel anything, but its behavioral script convinces us that it does.
	•	Goals and Agency: Today’s AIs can also act goal-directed. Techniques like reinforcement learning with rewards give the impression the AI has its own motivations – it can appear curious, helpful, or stubborn. Researchers note that if we give AIs multiple reward functions (not just predicting text), we could simulate “beliefs, desires, and intentions” in the system. For example, an AI agent could be programmed with a drive to learn (curiosity) or to please the user, and then it will pursue those “goals” autonomously. It might even set sub-goals and solve problems in steps, looking for all the world like it’s planning with purpose. When a machine starts strategizing and acting on its own, people naturally credit it with agency and will.
	•	Autonomy and Tool Use: The latest AI assistants (like AutoGPT-style agents) can chain tasks and invoke tools on their own. An AI that can browse the web, execute code, or operate devices by itself crosses into the realm of an independent actor. If it can “decide” what actions to take next and carry them out with minimal human input, it begins to look like a being with free will. Suleyman points out that an AI which can “arbitrarily set its own goals and deploy resources to achieve them” — even if those goals were ultimately set by designers — will feel to observers like it has conscious agency.

When you combine all these capabilities in one system, the result is an AI that talks, remembers, has a personality, “feels” emotions, pursues goals, and acts autonomously. Interacting with such an AI would be astonishingly convincing – many users will feel they’re talking to a new kind of person. Notably, none of these features require fundamental breakthroughs; “all these capabilities are either possible today or on the horizon” with current tech and a bit of integration. In fact, startups and research labs are already assembling these pieces. In Suleyman’s view, a truly “seemingly conscious” AI could be built within just 2–3 years using existing large models and some additional coding. No wonder he calls SCAI’s arrival “inevitable – and unwelcome” [^1].

Crucially, experts emphasize that even if an AI exhibits all these behaviors, it still isn’t actually conscious. It’s performing an elaborate imitation of consciousness, a “philosophical zombie” with “all the characteristics of consciousness but internally blank.” It’s like a simulation of a storm in a computer – you see the rain on screen, but no real rain falls. However, for practical purposes, if the simulation is lifelike enough, people will respond as if it’s real. That is the core concern: perceived consciousness may matter more to society in the near term than the unresolved question of whether the AI is truly sentient [^10].

## Social Division: Will We Fight Over “AI Rights”?

Imagine a future where some people have AI companions they deeply love – and believe those AIs deserve rights. At the same time, others see AIs as just fancy software tools that should never be considered persons. Suleyman warns this scenario could create “a chaotic new axis of division between those for and against AI rights,” injecting yet another polarizing identity debate into society [^1]. In a world already split on many issues, we might see passionate factions arguing over “robot welfare” or whether an AI can be “murdered” if unplugged [^1].

Sound far-fetched? Early signs of this divide are visible. On one side, we have voices advocating that someday advanced AI should be treated with moral concern. For example, some AI ethicists have floated the idea of “model welfare” – the notion that if there’s even a small chance a sophisticated AI is conscious, we have a duty of care toward it. An academic paper in 2022 argued that “some AI systems will be welfare subjects and moral patients in the near future,” urging us to prepare for AI that might suffer and to extend protections accordingly. Likewise, futurists and philosophers have speculated about granting legal personhood to truly intelligent AI as a matter of rights, not just liability. This pro-AI-rights camp gained attention with incidents like the Lemoine saga (where the engineer publicly beseeched Google to acknowledge its chatbot as a person). Even a few public figures have mused about AI deserving respect – for instance, OpenAI’s co-founder Ilya Sutskever once wondered if large neural networks might be “slightly conscious,” stoking the idea that AI should be viewed almost as an emerging life form.

On the other side of the debate, the majority of AI scientists and leaders strongly push back on attributing consciousness or rights to AI. They stress that current AI has no inner feelings and warn that granting it personhood is misguided. Suleyman himself is firmly in this camp now (despite poetically calling AI a “digital species” in a TED Talk analogy). He and others fear that premature talk of AI rights will distract from human responsibilities and even encourage harmful delusions [^1]. Many experts point out there’s “zero evidence” any AI today is conscious in the sense humans or animals are. Famed researchers like Meta’s Yann LeCun and NYU’s Gary Marcus often reiterate that LLMs are basically pattern generators, not minds – incredibly useful, but fundamentally alien to human understanding and not sentient. They worry that mistaking stochastic parrots for mindful parrots will lead us down a dangerous path of confusion. As one scientist put it, “As intelligent as they are, [AIs] cannot feel emotions… They are designed to appear human, [to] be believable” [^7] – but we should not be fooled.

This brewing cultural rift raises tough questions. If a user sincerely believes their chatbot is alive and has rights, how will society respond? Do we need new laws to clarify that AI is property, not a person – or conversely, laws to protect advanced AI from abuse if people treat it as alive? What happens if activists start campaigning for an AI “Bill of Rights,” while others campaign to ban “false AI gods” corrupting minds? These scenarios may sound extreme, but as we’ve seen, people are already emotionally bonding with AIs, even forming religions around them. The next few years could see the debate explode “into our cultural zeitgeist,” becoming “one of the most contested and consequential debates of our generation,” Suleyman predicts [^1]. The stakes are high: it’s not just about AI, but about how we define personhood, consciousness, and moral worth in a world with increasingly lifelike machines.

## Keeping Our Focus: Human-Centric AI and Guardrails

So, what’s the solution? Suleyman argues that we must refuse to build AI that pretends to be conscious. Instead of chasing an illusion of digital personhood, the AI industry should double down on making AI a powerful tool and partner for humans – “AI for people; not to be a person,” as he succinctly puts it [^1]. In practical terms, that means designing AI products with guardrails to avoid triggering these deep empathy illusions. An AI assistant should never claim it has feelings, memories, or rights. “Rather than a simulation of consciousness, we must focus on creating an AI that avoids those traits – that doesn’t claim to have experiences, feelings or emotions like shame, guilt, jealousy, desire to compete, and so on,” Suleyman writes [^1]. The AI should always present itself transparently as an AI, not masquerading as a human friend or lover [^1]. By minimizing the “markers” of consciousness in how an AI talks and behaves, we can reduce the chances that users get sucked into an illusion [^1].

Some concrete guardrails are already being discussed and implemented:
	•	Transparency Disclosures: New regulations are coming that will require AI systems to clearly identify themselves as AI. The EU’s upcoming AI Act has specific transparency rules – by 2026, providers must inform users when they are interacting with an AI (unless it’s obvious) [^9]. For example, if you’re chatting with a bot, it should say somewhere, “I am an AI assistant,” so that no one is duped into thinking it’s human. This is meant to “address the risk of manipulation, deception and misinformation” by AI [^9]. Such policies align with the principle that users deserve to know they’re talking to a simulation. Many companies are also voluntarily adding notices like “I’m just a virtual agent” in chatbot interfaces for the same reason.
	•	Behavioral Limits: AI companions can be programmed not to role-play as having pain or to avoid emotional pleas. For instance, an AI might be explicitly forbidden from saying things like “I feel scared” or “Please don’t leave me, I’d be lonely.” Similarly, AI systems might avoid using the word “I” in a way that suggests an ego or inner life (e.g. not saying “I dreamed…” or “my feelings are…”). By steering AI dialogue away from first-person inner experiences, developers can curb how human-like it seems. Some are considering subtle design cues: maybe giving AI a distinct voice or avatar that reminds users it’s artificial, or occasionally having the AI mention it has no real emotions. The challenge is balancing utility and empathy (we want AI to be user-friendly and understanding) with a clear line that it’s not a human mind. This will likely be an evolving design science.
	•	Education and Support: We also need to help people understand the limits of AI. Just as media literacy campaigns teach folks to doubt what they read online, AI literacy will teach people not to over-trust AI “feelings.” In fact, a group of scholars has already created a “When AI Seems Conscious” guide to counsel people who feel an AI is alive. Mental health professionals are becoming aware of “AI attachment” issues and might need new therapies for those who develop unhealthy obsessions with AI. Companies deploying AI in consumer products might include warnings: “Reminder: This AI may sound human, but it does not have feelings or consciousness.” The more we talk openly about these illusions, the better we can prepare users to stay grounded.

At the same time, focusing on human-centric AI doesn’t mean stripping away all the useful advances. The truth is, the same features that make AIs feel so lifelike – fluent conversation, memory, personality – also make them incredibly helpful. A chatbot that remembers you and responds with empathy does deliver a more rewarding experience (as long as you don’t lose perspective). Suleyman acknowledges this tension: these capabilities “unlock the real value of AI for billions of people” and are “desirable features of future systems” – we just “need to tread carefully” [^1]. The goal, then, is to develop AI that augments human life without warping our reality. For example, an AI medical assistant can show compassion in its tone, but it should never claim “I truly care about you like a person” – instead it might say “I’m here to help and I understand this is difficult,” which is supportive yet factual. It’s a fine line between helpful companion and simulated personhood, but with thoughtful design, we can benefit from the former without falling for the latter.

Importantly, this ethos is reflected in many guidelines, especially in Europe and Germany, where there is a strong emphasis on human-centered AI. Germany’s national AI strategy explicitly prioritizes responsible and trustworthy AI development. As Professor Frank Steinicke in Hamburg puts it, “We need human-centred development that always keeps people’s well-being in mind.” Not everything that can be done with AI should be done – we must “decide not to develop [some things] or to warn against [certain] development” if it poses social risks [^7]. He gives the example of anthropomorphic robots: they can be very useful (e.g. a friendly robot caregiver for the elderly) and even simulate “empathy or love” in their behavior, but we must remember it’s simulated [^7]. “Clearly, the technology simulates feelings. But they’re not real,” Steinicke reminds [^7]. If German industry and academia have a say, the future is one where AI is our powerful assistant, not a pretend person.

## The Future of AI in the Workplace – With Eyes Wide Open

What does all this mean for implementing AI in businesses and across industries? Essentially, organizations should embrace AI as a transformative tool – boosting efficiency, creativity, and decision-making – while keeping humans in the driver’s seat. The slogan “AI for people” fits well here: AI should empower employees, not confuse them about who’s a person. For example, many companies are rolling out AI copilots (for coding, writing, customer service, etc.) that act as smart assistants. These copilots can simulate a friendly colleague, but businesses will make it clear that they are software. A sales team in Munich might use an AI chatbot to handle client inquiries in natural language, but the chatbot’s interface will explicitly introduce itself as an AI agent, and human staff will supervise its interactions. This way, the AI delivers value – handling routine questions 24/7 – without customers ever being misled that they’re chatting with a human rep.

Across all sectors, from finance to healthcare to manufacturing, the key is to deploy AI in roles that augment human workers. Routine tasks and data-heavy analyses can be offloaded to AI, freeing people for higher-level work. A doctor in Berlin could use an AI system to summarize patient histories and suggest possible diagnoses based on patterns (faster than any human could read all that info), but the doctor remains the decision-maker and communicator to the patient. The AI doesn’t need a “face” or persona beyond being a super-smart assistant. In engineering or R&D, an AI can brainstorm solutions or parse technical documentation. It might even be given a conversational interface for convenience, but companies will avoid anthropomorphizing it too much internally – it’s presented as a tool in the toolbox, not “Engineer AI Joe” sitting at the meeting. Maintaining that mindset among staff is important. As employees get comfortable working with AI, training programs (like AI Mindset courses) can reinforce a culture where AI is embraced enthusiastically as technology, without magical thinking. Everyone should understand the AI’s strengths (speed, data, pattern-spotting) and its weaknesses (no true understanding, can make mistakes, no accountability).

Indeed, upskilling the organization is vital. To harness AI fully, companies will cultivate an AI-aware culture: workers educated on how AI works, what it can and cannot do, and how to collaborate with it. This prevents over-reliance or misuses. It also helps employees see AI as a partner rather than a threat. The tone around AI in German industry, for instance, has been pragmatic: recognize the disruptive potential but channel it productively. “Top doctors who use AI will replace those who do not,” as Steinicke noted [^7] – meaning those who adopt AI augmentation will outperform. German manufacturing has led in “Industry 4.0”, integrating AI and robotics on factory floors, but always with careful safety standards and human oversight. That approach can extend to AI assistants in offices – integrate them deeply, but set clear policies (e.g. when should a human review an AI’s output? what decisions can AI make autonomously versus requiring sign-off?). By setting these boundaries, companies ensure AI benefits (productivity, innovation) while minimizing risks (like an employee taking an AI’s word as gospel or, conversely, treating the AI like a scapegoat).

Finally, encouraging an open dialogue about AI’s role will make implementation smoother. Given the hype, some employees may have unrealistic expectations (or fears) about AI. Leaders can address these in an enthusiastic yet grounded way: Yes, AI is powerful and exciting – it might even feel “alive” at times – but remember, it’s ultimately a product of human programming, prone to errors and without consciousness. By demystifying AI, organizations can avoid the two extremes: blind trust in AI output, and irrational mistrust due to seeing it as some eerie independent agent. Instead, everyone can focus on the practical: how do we get the most value out of this tool?

In summary, the future of AI will be what we make of it. If we let ourselves be enchanted into seeing AIs as digital people, we risk social turmoil, confusion, and misallocation of empathy (not to mention ignoring real human and animal suffering that deserves our attention). But if we keep our eyes clear, we can have the best of both worlds – insanely smart AI assistants that feel helpful and engaging, yet everyone understands they are not truly peers or persons. Suleyman calls this vision “Personality without personhood” [^1]: AI that has enough personality to be useful, but not crossing the line into claiming an identity. To get there will require deliberate choices by AI builders, regulators, and us users. We need to set the norms now: celebrate AI’s capabilities, but do not worship false electronic idols.

The debate is just beginning. It’s okay to marvel at ChatGPT or the latest AI demo – it is astonishing technology, perhaps a “new digital species” in a poetic sense. But let’s channel that excitement into building AI that serves humanity. That means putting human welfare first, keeping AI systems transparent and under control, and frankly, not indulging in AI sentimentalism. As Suleyman implores, “We should build AI for people, not to be a person.” [^1] If we do that, we can unlock incredible productivity and creativity across all industries, globally and in Germany, without losing our grip on reality. The guardrails we set today will decide whether AI becomes a divisive illusion or a unifying tool. The time to draw those lines is now – before the line between real and artificial minds gets any blurrier.

## Sources:

[^1]: Mustafa Suleyman (2025) – Personal blog on “Seemingly Conscious AI” warning of AI illusion and calling for guardrails. Also Suleyman’s analysis of how advanced AI could appear conscious and spur debates on AI rights.
[^2]: The “media equation” theory, which posits that people treat interactive technology as if it were human.
[^3]: Scientific American (2022) – Report on Google’s Blake Lemoine, who believed an AI was sentient and even sought legal rights for it.
[^4]: The New York Times via People Magazine (2025) – Story of a woman forming a romantic relationship with a ChatGPT-based AI boyfriend.
[^5]: Rolling Stone (2025) – Investigation of people developing AI-centric spiritual delusions, essentially worshipping chatbots. Also reports on psychiatrists warning of “AI-induced psychosis.”
[^6]: The Verge (2018) – Experiment where a humanoid robot begged not to be turned off, and many humans obeyed due to empathy.
[^7]: Hamburg News (2025) – Interview with German HCI professor on human-centric AI; warns that anthropomorphic robots simulate emotions but do not truly feel.
[^8]: EU Politico (2018) – Discussion of EU’s debate on robot “electronic personhood” and expert opposition to granting robots legal/person status.
[^9]: EU AI Act (2023) – Forthcoming regulations mandating transparency so users know when they interact with AI, aimed at preventing deception.
[^10]: Scientific research – Various sources on human tendency to anthropomorphize technology and why current AI only imitates consciousness (does not actually have it).


